{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVaG71Po2qOD"
   },
   "source": [
    "# Redes Neuronales\n",
    "## Trabajo Práctico N° 1 - Ejercicio 1\n",
    "* Kammann, Lucas Agustín\n",
    "* Gaytan, Joaquín Oscar\n",
    "\n",
    "### Consultas y/o dudas\n",
    "* Métricas del modelo (Matriz de confusión, exactitud, precisión, etc.)\n",
    "* Procesamiento de los datos? (Filtrado)\n",
    "* CountVectorizer, es necesario que lo hagamos a mano o podemos utilizar la librería de sci-kit-learn?\n",
    "* Filtrado de numeros\n",
    "* Filtrado de palabras\n",
    "\n",
    "### Step by step\n",
    "* Obtención del dataset (separando entre \"train\" y \"test\")\n",
    "* Preprocesamiento de los datos\n",
    "* Distribuciones condicionales (Verosimilitudes/Likelihood)\n",
    "* Smoothing\n",
    "* Probabilidad a Priori\n",
    "* Clasificación (Naive Bayes Multinomial)\n",
    "* Clasificación usando log posteriori\n",
    "* Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripción del problema\n",
    "Se desea armar un clasificador de correos electrónicos o noticias en función del tópico sobre el cual trata dicho correo electrónico o noticia, para ello, se modela el problema considerando una variable aleatoria discreta $X$ con distribución categórica de $k_x = 20$ categorías, siendo estas los tópicos posibles que pueden tratar los documentos clasificados.\n",
    "\n",
    "$$X \\sim Categorica(p_1, p_2, ..., p_{k_x})$$\n",
    "\n",
    "Las características a analizar de los documentos para la clasificación, son las palabras. Se asume que el orden de las palabras no importa, que son independientes entre sí y que se encuentran identicamente distribuídas. De esta forma, se define una segunda variable aleatoria discreta $Y$ con distribución categórica de $k_y$ categorías, siendo estas las posibles palabras de nuestro vocabulario.\n",
    "\n",
    "$$Y \\sim Categorica(p_1, p_2, ..., p_{k_y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de la base de datos\n",
    "Usamos la librería **sklearn** para descargar la base de datos de entrenamiento y de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18842,
     "status": "ok",
     "timestamp": 1616464689182,
     "user": {
      "displayName": "JOAQUÍN OSCAR GAYTAN",
      "photoUrl": "",
      "userId": "15176508666834216481"
     },
     "user_tz": 180
    },
    "id": "NCv8VNLQ2k7R",
    "outputId": "270d983e-17a6-4a3a-c82f-f6e6a0ce6faf"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-80e22a2b411d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Loading the datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "# Loading the datasets\n",
    "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "# Categories\n",
    "categories = train.target_names\n",
    "\n",
    "# Train dataset, casting to numpy array\n",
    "train_data = np.array(train.data)\n",
    "train_target = np.array(train.target)\n",
    "train_size = len(train_data)\n",
    "\n",
    "# Test dataset, casting to numpy array\n",
    "test_data = np.array(test.data)\n",
    "test_target = np.array(test.target)\n",
    "test_size = len(test_data)\n",
    "\n",
    "# Logging useful information\n",
    "print(f'Dataset Train: {train_size} elements')\n",
    "print(f'Dataset Test: {test_size} elements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando la base de datos\n",
    "\n",
    "## Observando un correo electrónico de forma aleatoria\n",
    "Buscamos de forma aleatoria un elemento dentro de la base de datos de entrenamiento, con el objetivo de visualizar cómo suelen ser los correos electrónicos de noticias que se poseen, para tener una mejor comprensión de qué podemos llegar a encontrarnos dentro del vocabulario o el documento a analizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: wa2ise@cbnewsb.cb.att.com (robert.f.casey)\n",
      "Subject: 2 level brightness Xmas light set (for Easter?\n",
      "Keywords: xmas\n",
      "Organization: AT&T\n",
      "Lines: 54\n",
      "\n",
      "\n",
      "Yes, I know it's nowhere near Christmas time, but I'm gonna loose\n",
      "Net access in a few days (maybe a week or 2 if I'm lucky), and wanted\n",
      "to post this for interested people to save 'till Xmas.   :-(\n",
      "Note: Bell Labs is a good place IF you have a PhD and a good boss, I\n",
      "have neither.\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Subject: Xmas light set with 2 levels of brightness\n",
      "\n",
      "Another version of a variable brightness Xmas light set:\n",
      "This set starts with a 2 blinker 35 bulb string.  \n",
      "\n",
      "DIAGRAM:  orginal 2 way set\n",
      "\n",
      "120v---+--b-*-*-*-*-*-*-*--!\n",
      "       !---b-*-*-*-*-*-*-*-!\n",
      "                           !\n",
      "120rtn_____________________!\n",
      "\n",
      "modified set for 2 level brightness:\n",
      "\n",
      "                string 1\n",
      "120v---------*-*-*-*-*-*-*-*--!\n",
      "  \\_10K_______*-*-*-*-*-*-*-*-!\n",
      "    5w    !      string 2     !\n",
      "          b   ________________!\n",
      "120v rtn__!___!\n",
      "\n",
      "             ^ Note: no mods to wiring to the right of this point.\n",
      "\n",
      "Only one blinker is used.\n",
      "\n",
      "Note that the blinker would not have as much current thru it as the\n",
      "string 1 bulbs, because of the second string of bulbs in\n",
      "parallel with it.  That's why the use of the 10K 5W resistor\n",
      "here to add extra current thru the blinker to make up for the\n",
      "current shunted thru the second string while the blinker is glowing\n",
      "and the second string is not glowing.  When the blinker goes open,\n",
      "this resistor has only a slight effect on the brightness of the\n",
      "strings, s1 slightly dimmer, s2 slightly brighter.  \n",
      "Or use a 3W 120v bulb in place of the 10K resistor if you can get\n",
      "one.  Caution, do not replace with a standard C9 bulb, as these\n",
      "draw too much current and burn out the blinker.  C9 = approx 7W.\n",
      "\n",
      "What you'll see when it's working:  powerup, string 1 will light \n",
      "at full brightness, and b will be lit, bypassing most of the current \n",
      "from the second string, making them not light.  b will open, placing \n",
      "both strings in series, making the string that was out to glow at a \n",
      "low brightness, and the other string that was on before to glow \n",
      "at reduced brightness. \n",
      "\n",
      "Be sure to wire and insulate the splices, resistor leads, and cut wires \n",
      "in a safe manner!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_data = np.random.choice(train_data)\n",
    "print(random_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorías\n",
    "Dentro de la base de datos, los correos electrónicos que podemos encontrar pueden tratar de diferentes tópicos, y estos tópicos son las categorías de la variable aleatoria definida $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1616464712787,
     "user": {
      "displayName": "JOAQUÍN OSCAR GAYTAN",
      "photoUrl": "",
      "userId": "15176508666834216481"
     },
     "user_tz": 180
    },
    "id": "9Xh-PLNI7WsW",
    "outputId": "1665e32b-4e8b-491f-f4a2-d4115f937cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Show the news categories\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "      \"alt\": {\n",
      "            \"atheism\": {}\n",
      "      },\n",
      "      \"comp\": {\n",
      "            \"graphics\": {},\n",
      "            \"os\": {\n",
      "                  \"ms-windows\": {\n",
      "                        \"misc\": {}\n",
      "                  }\n",
      "            },\n",
      "            \"sys\": {\n",
      "                  \"ibm\": {\n",
      "                        \"pc\": {\n",
      "                              \"hardware\": {}\n",
      "                        }\n",
      "                  },\n",
      "                  \"mac\": {\n",
      "                        \"hardware\": {}\n",
      "                  }\n",
      "            },\n",
      "            \"windows\": {\n",
      "                  \"x\": {}\n",
      "            }\n",
      "      },\n",
      "      \"misc\": {\n",
      "            \"forsale\": {}\n",
      "      },\n",
      "      \"rec\": {\n",
      "            \"autos\": {},\n",
      "            \"motorcycles\": {},\n",
      "            \"sport\": {\n",
      "                  \"baseball\": {},\n",
      "                  \"hockey\": {}\n",
      "            }\n",
      "      },\n",
      "      \"sci\": {\n",
      "            \"crypt\": {},\n",
      "            \"electronics\": {},\n",
      "            \"med\": {},\n",
      "            \"space\": {}\n",
      "      },\n",
      "      \"soc\": {\n",
      "            \"religion\": {\n",
      "                  \"christian\": {}\n",
      "            }\n",
      "      },\n",
      "      \"talk\": {\n",
      "            \"politics\": {\n",
      "                  \"guns\": {},\n",
      "                  \"mideast\": {},\n",
      "                  \"misc\": {}\n",
      "            },\n",
      "            \"religion\": {\n",
      "                  \"misc\": {}\n",
      "            }\n",
      "      }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Processing the categories tree\n",
    "categories_tree = {}\n",
    "for category in categories:\n",
    "    # For each new category, the dot separated fields are converted into a list\n",
    "    category_levels = category.split('.')\n",
    "    \n",
    "    # Iteration over each field that describes a category, to create the corresponding\n",
    "    # tree path towards that node, each field should exist as a node in its corresponding level\n",
    "    categories_tree_node = categories_tree\n",
    "    for category_level in category_levels:\n",
    "            # If current category level has not been created already\n",
    "            if category_level not in categories_tree_node.keys():\n",
    "                categories_tree_node[category_level] = {}\n",
    "                \n",
    "            # Enter the category level\n",
    "            categories_tree_node = categories_tree_node[category_level]\n",
    "\n",
    "# Showing the result in a graphical view\n",
    "print(json.dumps(categories_tree, indent=6, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO! Agregar labels para cada posible categoria, más lindo el histograma y explicar la PROBABILIDAD A PRIORI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograma de tópicos\n",
    "Se calcula y grafica el histograma de los tópicos de las noticias o correos electrónicos utilizando la librería **matplotlib**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 4780,
     "status": "ok",
     "timestamp": 1616376899482,
     "user": {
      "displayName": "LUCAS KAMMANN",
      "photoUrl": "",
      "userId": "12661655261825067035"
     },
     "user_tz": 180
    },
    "id": "R7Jl_PbkC7CY",
    "outputId": "9fbc462e-28ed-4cc0-b7a7-9254af2e7597"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04242531 0.05161747 0.05223617 0.05214778 0.05108715 0.05241294\n",
      " 0.05170585 0.05250133 0.05285487 0.05276648 0.05303164 0.05258971\n",
      " 0.05223617 0.05250133 0.05241294 0.05294326 0.04825879 0.04984974\n",
      " 0.04109952 0.03332155]\n",
      "Wall time: 7.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Histogram for the categories in the training set\n",
    "priori, _, _ = plt.hist(train_target, bins=len(categories), range=(0,len(categories)), rwidth=0.5, density=True)\n",
    "plt.show()\n",
    "print(priori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the priori probability distribution: (20,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the priori probability distribution: {priori.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimación de probabilidades a priori\n",
    "Se computan las probabilidades a priori para las categorías, es decir, los tópicos. Se emplea una herramienta distinta a **matplotlib** para comparar el tiempo utilizada en el procesamiento. Se está estimando la siguiente función masa de probabilidad,\n",
    "$$P(X=x) ; x = 0, 1, ..., 19$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 998 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# For each category, filter the amount of occurences in the training dataset\n",
    "# the compute the priori probability\n",
    "priori_distribution = np.zeros(len(categories))\n",
    "for index, category in enumerate(categories):\n",
    "    frequency = (train_target == index).sum()\n",
    "    priori_distribution[index] = frequency\n",
    "priori_distribution /= priori_distribution.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VTY7lCEeMQWC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Importing and testing the CountVectorizer class of sklearn\n",
    "vectorizer = CountVectorizer(analyzer='word', stop_words='english')\n",
    "feature_matrix = vectorizer.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 129796)\n"
     ]
    }
   ],
   "source": [
    "print(feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de distribución de probabilidad condicional\n",
    "\n",
    "## Overfitting y Laplacian Smoothing\n",
    "El **overfitting** es un problema que ocurre cuando el modelo utilizado no es capaz de predecir correctamente frente a datos nuevos que no se encontraban en el conjunto de entrenamiento, lo cual le quita capacidad de generalizar, un aspecto fundamental en la resolución de problemas de inteligencia artificial. En este caso, para resolver el problema descripto se propone emplear un clasificador naive bayes multinomial, que puede traer apareado un problema de overfitting que se ilustra a continuación,\n",
    "\n",
    "### Discusiones sobre Laplacian Smoothing\n",
    "* https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece\n",
    "* https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    "* https://stats.stackexchange.com/questions/108797/in-naive-bayes-why-bother-with-laplace-smoothing-when-we-have-unknown-words-in\n",
    "* https://courses.cs.washington.edu/courses/cse446/20wi/Section7/naive-bayes.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "zbu8XISsgck7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Separate the matrix of documents (mails) and occurrences of words for each category\n",
    "# and compute the normalized distribution to get the likelihood for each category\n",
    "alpha = 1\n",
    "categories_distribution = np.zeros((len(categories), len(vectorizer.vocabulary_.keys())), dtype=np.longdouble)\n",
    "for index, category in enumerate(categories):\n",
    "  category_matrix = feature_matrix[train_target == index][:]\n",
    "  distribution = category_matrix.sum(axis=0) + alpha\n",
    "  categories_distribution[index,:] = distribution / distribution.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 129796)\n",
      "[[1.87142383e-05 1.63749585e-04 4.67855957e-06 ... 4.67855957e-06\n",
      "  4.67855957e-06 4.67855957e-06]\n",
      " [1.63359035e-04 8.40132181e-05 4.66740100e-06 ... 4.66740100e-06\n",
      "  4.66740100e-06 4.66740100e-06]\n",
      " [9.18720277e-05 2.97233031e-05 2.70211846e-06 ... 2.70211846e-06\n",
      "  2.70211846e-06 2.70211846e-06]\n",
      " ...\n",
      " [1.23957752e-04 7.54683962e-04 7.29163249e-06 ... 3.64581624e-06\n",
      "  3.64581624e-06 3.64581624e-06]\n",
      " [1.21906620e-04 2.98461034e-04 4.20367654e-06 ... 4.20367654e-06\n",
      "  4.20367654e-06 4.20367654e-06]\n",
      " [2.52340458e-05 9.08425648e-05 5.04680915e-06 ... 5.04680915e-06\n",
      "  5.04680915e-06 5.04680915e-06]]\n"
     ]
    }
   ],
   "source": [
    "print(categories_distribution.shape)\n",
    "print(categories_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando la prediccion con las probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:13: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Data, target and vectorizer\n",
    "input_size = 100\n",
    "input_data = test_data[:input_size]\n",
    "input_target = test_target[:input_size]\n",
    "input_matrix = vectorizer.transform(input_data)\n",
    "\n",
    "# Computing predictions for each input\n",
    "predictions = np.zeros(input_matrix.shape[0], dtype=int)\n",
    "for input_index in range(input_matrix.shape[0]):\n",
    "    # Computing the posteriori probability\n",
    "    # posteriori_unnormalized = ((categories_distribution * 10e3) ** input_matrix.toarray()[input_index]).prod(axis=1) * priori\n",
    "    posteriori_unnormalized = ((categories_distribution) ** input_matrix.toarray()[input_index]).prod(axis=1) * priori\n",
    "    posteriori_normalized = posteriori_unnormalized / posteriori_unnormalized.sum()\n",
    "\n",
    "    # Choosing the maximum posteriori probability as the prediction\n",
    "    predictions[input_index] = posteriori_unnormalized.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy obtained: 0.36\n"
     ]
    }
   ],
   "source": [
    "# Computing the accuracy of the model\n",
    "accuracy = (predictions == input_target).sum() / input_size\n",
    "print(f'Accuracy obtained: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculando la predicción usando log probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Log probabilities\n",
    "log_priori = np.log(priori)\n",
    "log_prob = np.log(categories_distribution)\n",
    "\n",
    "# Data, target and vectorizer\n",
    "input_size = 2200#test_size\n",
    "input_data = test_data[:input_size]\n",
    "input_target = test_target[:input_size]\n",
    "input_matrix = vectorizer.transform(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing the log posteriori probability\n",
    "log_posteriori = np.dot(log_prob, input_matrix.todense().transpose()) + (log_priori.reshape(-1, 1) * np.ones(input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Choosing the maximum log posteriori probability as the prediction\n",
    "predictions = np.zeros(input_matrix.shape[0], dtype=int)\n",
    "for input_index in range(input_size):\n",
    "    predictions[input_index] = log_posteriori[:,input_index].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy obtained: 0.8022727272727272\n"
     ]
    }
   ],
   "source": [
    "# Computing the accuracy of the model\n",
    "accuracy = (predictions == input_target).sum() / input_size\n",
    "print(f'Accuracy obtained: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP1-EJ1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "2af7b6151c4b8229b5656458602f56d7660010f5a78ab81e4e7a437e73cba351"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}