{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N° 1 - Ejercicio 2 - Notebook #2\n",
    "En esta segunda notebook, se busca definir cuál métrica es más apropiada para analizar la performance del modelo y qué hiper parámetros se van a utilizar para el ajuste del modelo acorde a la validación. Finalmente, estas decisiones se vuelcan en la selección del mejor modelo para el problema de la clasificación de correos electrónicos asociados grupos de noticias.\n",
    "\n",
    "### Fuentes útiles\n",
    "* https://en.wikipedia.org/wiki/Bessel%27s_correction\n",
    "* https://en.wikipedia.org/wiki/Kernel_density_estimation\n",
    "* https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "* https://stackoverflow.com/questions/58046129/can-someone-give-a-good-math-stats-explanation-as-to-what-the-parameter-var-smoo\n",
    "* https://scikit-learn.org/stable/modules/density.html\n",
    "\n",
    "### Integrantes del grupo\n",
    "* Gaytan, Joaquín Oscar\n",
    "* Kammann, Lucas Agustín"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Métrica\n",
    "La métrica a utilizar para cuantificar la performance de los modelos, seleccionar los hiperparámetros y validarlos, será la **sensibilidad** o **recall**.\n",
    "\n",
    "## 1.1. Justificación\n",
    "Se emplea el recall o sensibilidad respecto de los positivos, que se calcula como:\n",
    "$$recall = \\frac{TP}{TP+FN}$$\n",
    "Es decir, esta métrica da información sobre la proporción de positivos identificados sobre el total de positivos (reales). En el caso del diagnóstico de una enfermedad, nos interesa minimizar el número de falsos negativos, dado que una persona clasificada como negativo pero que efectivamente esté enferma puede no recibir el tratamiento correspondiente, empeorando su cuadro y poniendo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparación de los datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Cargando el dataset original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read database from .csv\n",
    "df = pd.read_csv('../assets/diabetes.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Sustitución de valores nulos\n",
    "Los valores nulos de las variables o características para la clasificación se reemplazan por NaN o Not a Number, para evitar que sean procesados en el análisis estadístico posterior, de esta forma luego serán reemplazados por algún estadístico. Se los remueve porque son valores inválidos acorde a la interpretación física de las variables diagnóstico con las cuales se realiza la clasificación y/o predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Glucose values\n",
    "df['Glucose'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Blood Pressure values\n",
    "df['BloodPressure'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Skin Thickness values\n",
    "df['SkinThickness'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Insulin values\n",
    "df['Insulin'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Body Mass Index values\n",
    "df['BMI'].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Filtrado de outliers\n",
    "\n",
    "Inicialmente se dejaron los outliers, pero se evidenció que en algunas iteraciones se producía una caída en la métrica,y bajo la suposición de que los outliers estaban afectando fuertemente a la estimación de los parámetros de los modelos de probabilidad, se los removió y como consecuencia mejoró la performance del modelo. En conclusión, es necesario eliminar estos valores porque son poco representativos de la población y afectan directamente a los modelos de probabilidad empleados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper import remove_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    remove_outliers(df, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Separación de datasets\n",
    "Se separa el dataset original en los datasets de train, valid y test. Además, se debe corregir que los valores inválidos del dataset original fueron reemplazados por el valor NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into the total train and the test datasets, because\n",
    "# the total train contains the train and valid datasets used for\n",
    "# hiper parameter selection\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Sustitución de valores inválidos\n",
    "Se filtran los valores inválidos de cada una de las variables, y se los reemplaza utilizando la media obtenida en el conjunto de entrenamiento. Particularmente, se opta por emplear la media de todo el conjunto de entrenamiento, para no introducir sesgo esencialmente dentro del conjunto empleado para la evaluación del modelo. Es necesario hacer esto para conseguir homogeneizar las muestras de cada varaibles de diagnóstico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\pandas\\core\\series.py:4563: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean of training\n",
    "train_means = train.mean().to_numpy()\n",
    "\n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(train.columns):\n",
    "    train.loc[:,column].replace(np.nan, train_means[index], inplace=True)\n",
    "\n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(test.columns):\n",
    "    test.loc[:,column].replace(np.nan, train_means[index], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Conjuntos de entrenamiento y evaluación\n",
    "Se construyen los conjuntos de entrenamiento y evaluación, y además se filtran aquellas variables de interés.\n",
    "\n",
    "### Remoción de **DiabetesPedigreeFunction** \n",
    "Se elimina la variable **DiabetesPedigreeFunction** dado que el análisis estadístico previamente realizado dejaba entrever que la información que aporta es baja. Esto lo indicaron dos factores, en primer lugar el poco cambio entre las distribuciones condicionadas, y en segundo lugar la baja correlación con la variable de salida.\n",
    "\n",
    "### Remoción de **BloodPressure**\n",
    "Se elimina la variable **BloodPressure** dado que el análisis estadístico previamente realizado mostró que la información que aporta es baja. Esto lo indicador dos factores, en primer lugar el poco cambio entre las distribuciones condicionadas, y en segundo lugar, la baja correlación con la variable de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global filter of the variables\n",
    "global_variables_filter = np.array([True, True, False, True, True, True, False, True, False])\n",
    "global_variables_count = global_variables_filter.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the inputs and outputs of the train dataset\n",
    "x_train = train.to_numpy()[:,global_variables_filter]\n",
    "y_train = train.to_numpy()[:,8]\n",
    "\n",
    "# Extracting the inputs and outputs of the test dataset\n",
    "x_test = test.to_numpy()[:,global_variables_filter]\n",
    "y_test = test.to_numpy()[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.670412</td>\n",
       "      <td>122.132959</td>\n",
       "      <td>72.143713</td>\n",
       "      <td>28.953003</td>\n",
       "      <td>131.703297</td>\n",
       "      <td>32.373372</td>\n",
       "      <td>0.421482</td>\n",
       "      <td>32.758491</td>\n",
       "      <td>0.331471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.229410</td>\n",
       "      <td>30.544791</td>\n",
       "      <td>10.935330</td>\n",
       "      <td>8.451481</td>\n",
       "      <td>52.528972</td>\n",
       "      <td>6.382944</td>\n",
       "      <td>0.242506</td>\n",
       "      <td>11.179596</td>\n",
       "      <td>0.471181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>72.143713</td>\n",
       "      <td>28.953003</td>\n",
       "      <td>131.703297</td>\n",
       "      <td>32.373372</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>131.703297</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.551000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   537.000000  537.000000     537.000000     537.000000  537.000000   \n",
       "mean      3.670412  122.132959      72.143713      28.953003  131.703297   \n",
       "std       3.229410   30.544791      10.935330       8.451481   52.528972   \n",
       "min       0.000000   44.000000      40.000000       7.000000   14.000000   \n",
       "25%       1.000000  100.000000      64.000000      25.000000  116.000000   \n",
       "50%       3.000000  118.000000      72.143713      28.953003  131.703297   \n",
       "75%       6.000000  141.000000      80.000000      32.000000  131.703297   \n",
       "max      13.000000  199.000000     104.000000      56.000000  342.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  537.000000                537.000000  537.000000  537.000000  \n",
       "mean    32.373372                  0.421482   32.758491    0.331471  \n",
       "std      6.382944                  0.242506   11.179596    0.471181  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.600000                  0.238000   24.000000    0.000000  \n",
       "50%     32.373372                  0.361000   29.000000    0.000000  \n",
       "75%     36.600000                  0.551000   40.000000    1.000000  \n",
       "max     50.000000                  1.191000   66.000000    1.000000  "
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>231.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>231.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.054850</td>\n",
       "      <td>120.659160</td>\n",
       "      <td>72.055707</td>\n",
       "      <td>28.837352</td>\n",
       "      <td>133.156891</td>\n",
       "      <td>31.824214</td>\n",
       "      <td>0.448196</td>\n",
       "      <td>32.911329</td>\n",
       "      <td>0.389610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.354694</td>\n",
       "      <td>30.222273</td>\n",
       "      <td>10.753877</td>\n",
       "      <td>7.795778</td>\n",
       "      <td>49.215393</td>\n",
       "      <td>6.471815</td>\n",
       "      <td>0.249987</td>\n",
       "      <td>10.758273</td>\n",
       "      <td>0.488721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>131.703297</td>\n",
       "      <td>27.250000</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.953003</td>\n",
       "      <td>131.703297</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.500000</td>\n",
       "      <td>138.500000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>131.703297</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>1.144000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   231.000000  231.000000     231.000000     231.000000  231.000000   \n",
       "mean      4.054850  120.659160      72.055707      28.837352  133.156891   \n",
       "std       3.354694   30.222273      10.753877       7.795778   49.215393   \n",
       "min       0.000000   56.000000      44.000000       7.000000   18.000000   \n",
       "25%       1.000000   99.000000      64.500000      25.500000  131.703297   \n",
       "50%       3.000000  115.000000      72.000000      28.953003  131.703297   \n",
       "75%       6.500000  138.500000      80.000000      32.000000  131.703297   \n",
       "max      13.000000  198.000000     104.000000      48.000000  360.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  231.000000                231.000000  231.000000  231.000000  \n",
       "mean    31.824214                  0.448196   32.911329    0.389610  \n",
       "std      6.471815                  0.249987   10.758273    0.488721  \n",
       "min     18.200000                  0.084000   21.000000    0.000000  \n",
       "25%     27.250000                  0.251500   24.000000    0.000000  \n",
       "50%     32.000000                  0.389000   30.000000    0.000000  \n",
       "75%     35.750000                  0.646000   40.000000    1.000000  \n",
       "max     49.600000                  1.144000   65.000000    1.000000  "
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Selección, validación y evaluación de modelos\n",
    "Para poder realizar la validación del modelo y escoger aquellos hiper parámetros que obtienen la mejor performace según la métrica, se utiliza el método de k-folding, dado que se cuenta con una cantidad de datos pequeña. Si no se decidiera utilizar k-folding, al tener un conjunto de validación tan pequeño o reducido, la varianza en el estimador de la métrica es demasiado grande y la estimación posee mucho ruido, lo cual es un problema porque provocaría elegir un modelo equivocado.\n",
    "\n",
    "Este algoritmo está implementado dentro de la función **GridSearchCV**, que además busca optimizar la métrica elegida (recall) variando los hiper parámetros del modelo.\n",
    "\n",
    "Finalmente, se entrena al modelo con los hiper parámetros que resultan de este análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Modelo utilizando Gaussian Naive Bayes sin hiper parámetros\n",
    "En este punto, se busca entrenar el model y evaluar su performance, contrastando los resultados obtenidos por la implementación propia con la implementación de sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Versión del clasificador propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gaussian_naive_bayes import BinaryGaussianNaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the found model with the complete train set\n",
    "classifier = BinaryGaussianNaiveBayes()\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using the test dataset and computing the score\n",
    "predictions = classifier.predict(x_test)\n",
    "score = recall_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the model 0.644\n"
     ]
    }
   ],
   "source": [
    "print(f'Score of the model {np.round(score, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Versión del clasificador sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the model 0.644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create and train the model\n",
    "c = GaussianNB()\n",
    "c.fit(x_train, y_train)\n",
    "\n",
    "# Predict and compute score\n",
    "p = c.predict(x_test)\n",
    "s = recall_score(y_test, p)\n",
    "\n",
    "# Show the resulting score\n",
    "print(f'Score of the model {np.round(s, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Modelo utilizando Gaussian Naive Bayes con hiper parámetros\n",
    "\n",
    "### Smoothing\n",
    "En las variables aleatorias continuas con distribución normal o gaussiana, cuando es estiman sus parámetros mediante un conjunto de entrenamiento, se corre el riesgo de **underfitting** y **overfitting**, es decir, que una mala estimación de la distribución de probabilidad puede excluir valores de su rango que luego hacen que el modelo no pueda predecir con éxito. Entonces, una técnica empleada para solucionar este tipo de problemas, consiste en agregar un suavizado en las variables gaussianas, sumando un término al desvío estándar.\n",
    "\n",
    "### Corrección de Bessel\n",
    "En la estadística, los estadísticos se definen como funciones que se aplican sobre muestras y datos, y un caso particular de ellos son los estimadores que se emplean para estimar parámetros poblacionales como la media o el desvío estándar en este caso. Una característica buscada de los estimadores, es que sean sin sesgo, para ello el estimador de la varianza debe ser,\n",
    "\n",
    "$$ s^{2} = \\frac{1}{n-1} \\cdot \\sum_{i=1}^{n}(x_i - x_{mean})^{2}$$\n",
    "\n",
    "Y, por lo general, a veces se calcula\n",
    "\n",
    "$$ s^{2} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n}(x_i - x_{mean})^{2}$$\n",
    "\n",
    "Entonces, la **corrección de Bessel** es un factor multiplicativo que corrige el término del denominador.\n",
    "\n",
    "### Filtro de variables\n",
    "El filtro de variables consiste en agregar un hiper parámetro que le permite al modelo escoger qué variables usar, y de esta forma, encontrar aquella combinación con la cual la predicción se optimiza. En principio, se sabe que algunas variables guardan una correlación medianamente fuerte, y que otras no ven su distribución muy afectada por la clase a la que pertenezca el individuo, por ello, resulta razonable que existan modelos que no empleen la totalidad de las variables o características que se proveen en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gaussian_naive_bayes import BinaryGaussianNaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Declaring the hiper parameters and their values for the GridSearchCV\n",
    "# to search the best model according to the performance\n",
    "parameters = {\n",
    "    'std_smoothing': [0, 0.1, 1],\n",
    "    'std_correction': [False, True],\n",
    "    'filter_variables': list(itertools.product([True, False], repeat=global_variables_count)),\n",
    "}\n",
    "\n",
    "# Estimator or model\n",
    "estimator = BinaryGaussianNaiveBayes()\n",
    "\n",
    "# Search the best model\n",
    "grid = GridSearchCV(estimator, parameters, cv=10, scoring='recall', n_jobs=-1)\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the found model with the complete train set\n",
    "classifier = BinaryGaussianNaiveBayes(\n",
    "    std_smoothing=grid.best_params_['std_smoothing'], \n",
    "    std_correction=grid.best_params_['std_correction'], \n",
    "    filter_variables=grid.best_params_['filter_variables']\n",
    ")\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using the test dataset and computing the score\n",
    "predictions = classifier.predict(x_test)\n",
    "score = recall_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Score of the model {np.round(score, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Modelo Naive Bayes con distribución por variable\n",
    "\n",
    "A partir del análisis estadístico realizado sobre las variables se sacó la conclusión de que en la realidad algunas variables o características físicas del problema utilizadas para la clasificación poseen una distribución que se aparta bastante del modelo probabilístico de una variable gaussiana. Entonces, en vista de esta observación, se desea poder replicar el concepto del clasificador **Gaussian Naive Bayes**, pero dando la libertad de que cada variable trabaje con una distribución propia.\n",
    "\n",
    "En primera instancia, se implementan las distribuciones **gaussiana** y **exponencial**.\n",
    "\n",
    "*Nota: Se excluyeron los hiper parámetros que pueden estar más relacionados a una distribución que otra, principalmente para no complejizar la búsqueda del modelo óptimo y probar en una primera instancia diferentes distribuciones para cada variable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mixed_naive_bayes import BinaryMixedNaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Declaring the hiper parameters and their values for the GridSearchCV\n",
    "# to search the best model according to the performance\n",
    "parameters = {\n",
    "    'filter_variables': list(itertools.product([True], repeat=8)),\n",
    "    'variables_models': [\n",
    "        # Pregnancies | Glucose | BloodPressure | SkinThickness | Insulin | BMI | DiabetesPedigreeFunction | Age\n",
    "        [ 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian'],\n",
    "        [ 'exponential', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'gaussian', 'exponential', 'exponential'],\n",
    "        [ 'exponential', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'gaussian', 'exponential', 'gaussian'],\n",
    "        [ 'exponential', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'gaussian', 'gaussian', 'exponential'],\n",
    "        [ 'exponential', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'gaussian', 'gaussian', 'gaussian'],\n",
    "        [ 'exponential', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'exponential'],\n",
    "        [ 'exponential', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'gaussian'],\n",
    "        [ 'exponential', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'exponential'],\n",
    "        [ 'exponential', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian'],\n",
    "        [ 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'gaussian', 'exponential', 'exponential'],\n",
    "        [ 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'gaussian', 'exponential', 'gaussian'],\n",
    "        [ 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'gaussian', 'gaussian', 'exponential'],\n",
    "        [ 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'gaussian', 'gaussian', 'gaussian'],\n",
    "        [ 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'exponential'],\n",
    "        [ 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'exponential', 'gaussian'],\n",
    "        [ 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'exponential'],\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Estimator or model\n",
    "estimator = BinaryMixedNaiveBayes()\n",
    "\n",
    "# Search the best model\n",
    "grid = GridSearchCV(estimator, parameters, cv=10, scoring='recall', n_jobs=-1)\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the found model with the complete train set\n",
    "classifier = BinaryMixedNaiveBayes(\n",
    "    variables_models=grid.best_params_['variables_models'], \n",
    "    filter_variables=grid.best_params_['filter_variables']\n",
    ")\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using the test dataset and computing the score\n",
    "predictions = classifier.predict(x_test)\n",
    "score = recall_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Score of the model {np.round(score, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Modelo utilizando Kernel Density Estimator\n",
    "A fin de comparar el desempeño del clasificador gaussiano, se propone el uso de un clasificador con KDE (Kernel Density Estimation). Este es un método para estimar funciones de densidad de probabilidad que se basa en la información de las muestras en un entorno de valor determinado. La estimación se define como:\n",
    "\n",
    "$$\\hat{f}(x) = \\frac{1}{nh} \\sum_{i=1}^n{K\\left(\\frac{x-x_i}{h}\\right)}$$\n",
    "\n",
    "Donde $K(x)$ es una función no negativa denominada **kernel** y $n$ representa el número de observaciones. Además, $h$ es un parámetro positivo llamado **bandwidth** que se vincula con la cantidad de puntos a tener en cuenta para la estimación de un valor puntual de $\\hat{f}$. Para el caso particular de este clasificador se emplean dos tipos de funciones kernel: ‘gaussian’, ‘tophat’, ‘epanechnikov’, ‘exponential’, ‘linear’, ‘cosine’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.kde_naive_bayes import BinaryKDENaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Declaring the hiper parameters and their values for the GridSearchCV\n",
    "# to search the best model according to the performance\n",
    "parameters = {\n",
    "    'kernel': ['exponential', 'tophat', 'linear', 'cosine', 'gaussian', 'epanechnikov'],\n",
    "    'bandwidth': [0.01, 0.1, 1, 10],\n",
    "    'filter_variables': list(itertools.product([True, False], repeat=8)),\n",
    "}\n",
    "\n",
    "# Estimator or model\n",
    "estimator = BinaryKDENaiveBayes()\n",
    "\n",
    "# Search the best model\n",
    "grid = GridSearchCV(estimator, parameters, cv=10, scoring='recall', n_jobs=-1)\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the found model with the complete train set\n",
    "classifier = BinaryKDENaiveBayes(\n",
    "    kernel=grid.best_params_['kernel'], \n",
    "    bandwidth=grid.best_params_['bandwidth'], \n",
    "    filter_variables=grid.best_params_['filter_variables']\n",
    ")\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using the test dataset and computing the score\n",
    "predictions = classifier.predict(x_test)\n",
    "score = recall_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Score of the model {np.round(score, 3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
