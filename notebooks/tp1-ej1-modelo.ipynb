{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N° 1 - Notebook #2\n",
    "En esta segunda notebook, se busca definir cuál métrica es más apropiada para analizar la performance del modelo y qué hiper parámetros se van a utilizar para el ajuste del modelo acorde a la validación. Finalmente, estas decisiones se vuelcan en la selección del mejor modelo para el problema de la clasificación de correos electrónicos asociados grupos de noticias.\n",
    "\n",
    "### Consideraciones para ejecutar notebook\n",
    "* La sección de preprocesamiento previo dentro de la definición de hiper parámetros sólo debe ejecutarse una vez, si ya se poseen los datasets preprocesados, no es necesario.\n",
    "\n",
    "### Integrantes del grupo\n",
    "* Kammann, Lucas Agustín\n",
    "* Gaytan, Joaquín Oscar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Métrica\n",
    "La métrica a utilizar para cuantificar la performance de los modelos, seleccionar los hiperparámetros y validarlos será la **exactitud** o **accuracy**.\n",
    "\n",
    "## 1.1 Justificación\n",
    "La problemática a resolver tiene por objetivo asegurar clasificar entre múltiples clases, por ende, el objetivo es acertar la mayor cantidad de predicciones posibles. Para este tipo de problemas, conviene usar la exactitud, pero antes es necesario comprobar que la distribución de clases está balanceada o es uniforme, dado que si no fuera así (o aproximadamente así) entonces habría un sesgo en la estimación. Esto último se debe a que no seríamos capaces de cuantificar realmente lo malo que el modelo es prediciendo aquellas clases minoritarias.\n",
    "\n",
    "Es decir, si bien es una métrica acorde al problema, cuando las clases no están balanceadas su interpretación numérica no es realista. En esos casos, se puede utilizar el promedio de la sensibilidad de cada clase, porque dicha sensibilidad representa la probabilidad de acertar en la predicción dada cada clase y si luego las promediamos estamos ponderando de igual forma cada clase.\n",
    "\n",
    "En conclusión, dado lo que se observó en previos análisis **(ver Notebook #1)**, se puede asumir que la distribución de clases es aproximadamente uniforme por lo cual la exactitud es una métrica aceptable. Si se quisiera obtener una cantidad más realista, con el promedio de sensibilidad se cumpliría tal objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hiper parámetros\n",
    "Se consideran hiper parámetros aquellos que se determinan de manera óptima eligiendo aquel que da mejor resultado en el conjunto de datos de validación, donde muchos modelos con diversos tipos y valores de hiper parámetros compiten por ver cuál obtuvo la mejor medida de performance, es decir, de la métrica. Para ello, consideraremos como hiper parámetros los siguientes aspectos,\n",
    "\n",
    "* Algoritmo de preprocesamiento empleado: Stemming, Lemmatization, Ninguno\n",
    "* Filtrado por stop words\n",
    "* Vectorizer empleado: CountVectorizer, TfIdfVectorizer\n",
    "* Modelo empleado: MultinomialNaiveBayes, OneVsRestClassifier\n",
    "* Coeficiente de Laplacian Smoothing\n",
    "* Coeficiente de mínima frecuencia por documentos\n",
    "* Coeficiente de máxima frecuencia por documentos\n",
    "\n",
    "**NOTA**, sólo correr las siguientes celdas de preprocesamiento, si no se poseen los '*.txt' preprocesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Preprocesamiento previo\n",
    "A continuación, se corren los algoritmos de procesamiento directamente sobre el conjunto de entrenamiento para evitar tener que realizar el preprocesamiento cada vez teniendo en cuenta que no existen más opciones a analizar. Es decir, se dejan preparados los procesamientos de texto más costosos computacionalmente, para que luego el proceso de entrenamiento y validación no requiera realizarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Descargando los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Lucas A.\n",
      "[nltk_data]     Kammann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lucas A.\n",
      "[nltk_data]     Kammann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Train: 11314 elements\n",
      "Dataset Train: 7532 elements\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Loading datasets\n",
    "train = fetch_20newsgroups(\n",
    "    subset='train', \n",
    "    shuffle=True, \n",
    "    remove=('headers', 'footers')\n",
    ")\n",
    "\n",
    "test = fetch_20newsgroups(\n",
    "    subset='test', \n",
    "    shuffle=True, \n",
    "    remove=('headers', 'footers')\n",
    ")\n",
    "\n",
    "# Casting\n",
    "train_raw_input = np.array(train.data)\n",
    "train_output = np.array(train.target)\n",
    "train_size = len(train_raw_input)\n",
    "\n",
    "test_raw_input = np.array(test.data)\n",
    "test_output = np.array(test.target)\n",
    "test_size = len(test_raw_input)\n",
    "\n",
    "# Logging useful information\n",
    "print(f'Dataset Train: {train_size} elements')\n",
    "print(f'Dataset Test: {test_size} elements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Guardando los datasets originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normal trainning dataset has been saved in the local storage system.\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the structure of the trainning data set for the non-processed case\n",
    "normal_train = {\n",
    "    'input': train_raw_input,\n",
    "    'output': train_output\n",
    "}\n",
    "\n",
    "# Save with pickle\n",
    "with open('tp1_ej1_train_normal.txt', 'wb') as file:\n",
    "    pickle.dump(normal_train, file)\n",
    "\n",
    "# Logging\n",
    "print('The normal trainning dataset has been saved in the local storage system.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the structure of the trainning data set for the non-processed case\n",
    "normal_test = {\n",
    "    'input': test_raw_input,\n",
    "    'output': test_output\n",
    "}\n",
    "\n",
    "# Save with pickle\n",
    "with open('tp1_ej1_test_normal.txt', 'wb') as file:\n",
    "    pickle.dump(normal_test, file)\n",
    "\n",
    "# Logging\n",
    "print('The normal trainning dataset has been saved in the local storage system.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Guardando los datasets con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the stemmer instance\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming pre processing of the tran dataset finished.\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Process and save the stemmed trainning data\n",
    "stemmed_train_raw_input = []\n",
    "for document in train_raw_input:\n",
    "    tokens = word_tokenize(document)\n",
    "    new_document = \" \".join([stemmer.stem(token.lower()) for token in tokens if token.isalpha()])\n",
    "    stemmed_train_raw_input.append(new_document)\n",
    "\n",
    "# Logging\n",
    "print('Stemming pre processing of the tran dataset finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemming trainning dataset has been saved in the local storage system.\n",
      "Wall time: 17.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the structure of the trainning data set for the stemming case\n",
    "stemmed_train = {\n",
    "    'input': stemmed_train_raw_input,\n",
    "    'output': train_output\n",
    "}\n",
    "\n",
    "# Save with pickle\n",
    "with open('tp1_ej1_train_stemmed.txt', 'wb') as file:\n",
    "    pickle.dump(stemmed_train, file)\n",
    "\n",
    "# Logging\n",
    "print('The stemming trainning dataset has been saved in the local storage system.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming pre processing of the test dataset finished.\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Process and save the stemmed trainning data\n",
    "stemmed_test_raw_input = []\n",
    "for document in train_raw_input:\n",
    "    tokens = word_tokenize(document)\n",
    "    new_document = \" \".join([stemmer.stem(token.lower()) for token in tokens if token.isalpha()])\n",
    "    stemmed_test_raw_input.append(new_document)\n",
    "\n",
    "# Logging\n",
    "print('Stemming pre processing of the test dataset finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemming trainning dataset has been saved in the local storage system.\n",
      "Wall time: 23.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the structure of the trainning data set for the stemming case\n",
    "stemmed_test = {\n",
    "    'input': stemmed_test_raw_input,\n",
    "    'output': test_output\n",
    "}\n",
    "\n",
    "# Save with pickle\n",
    "with open('tp1_ej1_test_stemmed.txt', 'wb') as file:\n",
    "    pickle.dump(stemmed_test, file)\n",
    "\n",
    "# Logging\n",
    "print('The stemming trainning dataset has been saved in the local storage system.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Guardando los datasets con lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the lemmatizer instance\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization pre processing of the train dataset finished.\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Process and save the lemmatized trainning data\n",
    "lemmatized_train_raw_input = []\n",
    "for document in train_raw_input:\n",
    "    tokens = word_tokenize(document)\n",
    "    new_document = \" \".join([lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha()])\n",
    "    lemmatized_train_raw_input.append(new_document)\n",
    "\n",
    "# Logging\n",
    "print('Lemmatization pre processing of the train dataset finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatization trainning dataset has been saved in the local storage system.\n",
      "Wall time: 25.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the structure of the trainning data set for the lemmatization case\n",
    "lemmatization_train = {\n",
    "    'input': lemmatized_train_raw_input,\n",
    "    'output': train_output\n",
    "}\n",
    "\n",
    "# Save with pickle\n",
    "with open('tp1_ej1_train_lemmatization.txt', 'wb') as file:\n",
    "    pickle.dump(lemmatization_train, file)\n",
    "\n",
    "# Logging\n",
    "print('The lemmatization trainning dataset has been saved in the local storage system.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization pre processing of the test dataset finished.\n",
      "Wall time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Process and save the lemmatized trainning data\n",
    "lemmatized_test_raw_input = []\n",
    "for document in train_raw_input:\n",
    "    tokens = word_tokenize(document)\n",
    "    new_document = \" \".join([lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha()])\n",
    "    lemmatized_test_raw_input.append(new_document)\n",
    "\n",
    "# Logging\n",
    "print('Lemmatization pre processing of the test dataset finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatization trainning dataset has been saved in the local storage system.\n",
      "Wall time: 20 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the structure of the trainning data set for the lemmatization case\n",
    "lemmatization_test = {\n",
    "    'input': lemmatized_test_raw_input,\n",
    "    'output': test_output\n",
    "}\n",
    "\n",
    "# Save with pickle\n",
    "with open('tp1_ej1_test_lemmatization.txt', 'wb') as file:\n",
    "    pickle.dump(lemmatization_test, file)\n",
    "\n",
    "# Logging\n",
    "print('The lemmatization trainning dataset has been saved in the local storage system.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparación de datasets\n",
    "En este paso, previo al entrenamiento, selección y validación de los modelos. Es necesario, primero, cargar todos los datasets y, luego, separar entre tres subconjuntos definidos como **train**, **valid** y **test**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Cargando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the preprocessed datasets (test and train), load all existing\n",
    "# variants, normal/original, lemmatized and stemmed. The underscore train variables\n",
    "# is because it's not exactly the train dataset because it will be splitted into\n",
    "# the actual training set and the validation set.\n",
    "\n",
    "with open('tp1_ej1_train_normal.txt', 'rb') as file:\n",
    "    _train_normal = pickle.load(file)\n",
    "    \n",
    "with open('tp1_ej1_train_stemmed.txt', 'rb') as file:\n",
    "    _train_stemmed = pickle.load(file)\n",
    "    \n",
    "with open('tp1_ej1_train_lemmatization.txt', 'rb') as file:\n",
    "    _train_lemmatization = pickle.load(file)\n",
    "\n",
    "with open('tp1_ej1_test_normal.txt', 'rb') as file:\n",
    "    test_normal = pickle.load(file)\n",
    "    \n",
    "with open('tp1_ej1_test_stemmed.txt', 'rb') as file:\n",
    "    test_stemmed = pickle.load(file)\n",
    "    \n",
    "with open('tp1_ej1_test_lemmatization.txt', 'rb') as file:\n",
    "    test_lemmatization = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Separando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting into train and validation\n",
    "train_normal_input, valid_normal_input, train_normal_output, valid_normal_output = \\\n",
    "    train_test_split(_train_normal['input'], _train_normal['output'], test_size=0.2, random_state=14)\n",
    "\n",
    "train_stemmed_input, valid_stemmed_input, train_stemmed_output, valid_stemmed_output = \\\n",
    "    train_test_split(_train_stemmed['input'], _train_stemmed['output'], test_size=0.2, random_state=14)\n",
    "\n",
    "train_lemmatization_input, valid_lemmatization_input, train_lemmatization_output, valid_lemmatization_output = \\\n",
    "    train_test_split(_train_lemmatization['input'], _train_lemmatization['output'], test_size=0.2, random_state=14)\n",
    "\n",
    "# Better formatting\n",
    "_train = {\n",
    "    'normal': _train_normal,\n",
    "    'stemmed': _train_stemmed,\n",
    "    'lemmatization': _train_lemmatization\n",
    "}\n",
    "\n",
    "train = {\n",
    "    'normal': { 'input': train_normal_input, 'output': train_normal_output },\n",
    "    'stemmed': { 'input': train_stemmed_input, 'output': train_stemmed_output },\n",
    "    'lemmatization': { 'input': train_lemmatization_input, 'output': train_lemmatization_output}\n",
    "}\n",
    "\n",
    "valid = {\n",
    "    'normal': { 'input': valid_normal_input, 'output': valid_normal_output },\n",
    "    'stemmed': { 'input': valid_stemmed_input, 'output': valid_stemmed_output },\n",
    "    'lemmatization': { 'input': valid_lemmatization_input, 'output': valid_lemmatization_output }\n",
    "}\n",
    "        \n",
    "test = {\n",
    "    'normal': test_normal,\n",
    "    'stemmed': test_stemmed,\n",
    "    'lemmatization': test_lemmatization\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Entrenamiento y selección de hiper parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 27min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from src.multinomial_naive_bayes import MultinomialNaiveBayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Creating two lists, saving the hiperparameters and the score for the model\n",
    "model_params = []\n",
    "model_score = []\n",
    "\n",
    "for use_algorithm in ['normal', 'stemmed', 'lemmatization']:\n",
    "    for use_vectorizer in ['TfidfVectorizer', 'CountVectorizer']:\n",
    "        for use_smooth_idf in ([False, True] if use_vectorizer == 'TfidfVectorizer' else [None]):\n",
    "            for use_sublinear_tf in ([False, True] if use_vectorizer == 'TfidfVectorizer' else [None]):\n",
    "                for use_stop_words in [None, 'english']:\n",
    "                    for use_min_df in [1, 2, 0.0001, 0.001, 0.01, 0.1]:\n",
    "                        for use_max_df in [0.15, 0.2, 0.3, 0.4, 0.5, 1.0]:\n",
    "                            # Creating the vectorizer\n",
    "                            if use_vectorizer == 'CountVectorizer':\n",
    "                                vectorizer = CountVectorizer(\n",
    "                                    stop_words=use_stop_words, \n",
    "                                    min_df=use_min_df, \n",
    "                                    max_df=use_max_df\n",
    "                                )\n",
    "                            elif use_vectorizer == 'TfidfVectorizer':\n",
    "                                vectorizer = TfidfVectorizer(\n",
    "                                    stop_words=use_stop_words, \n",
    "                                    min_df=use_min_df,\n",
    "                                    max_df=use_max_df,\n",
    "                                    smooth_idf=use_smooth_idf,\n",
    "                                    sublinear_tf=use_sublinear_tf\n",
    "                                )\n",
    "\n",
    "                            # Processing both the training and the validation datasets\n",
    "                            x_train = vectorizer.fit_transform(train[use_algorithm]['input'])\n",
    "                            y_train = train[use_algorithm]['output']\n",
    "                            x_valid = vectorizer.transform(valid[use_algorithm]['input'])\n",
    "                            y_valid = valid[use_algorithm]['output']\n",
    "\n",
    "                            # Run training and validation routines\n",
    "                            for use_alpha in [0, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175, 0.1, 1]:\n",
    "                                # Creating and training the model\n",
    "                                classifier = MultinomialNaiveBayes(alpha=use_alpha)\n",
    "                                classifier.fit(x_train, y_train)\n",
    "\n",
    "                                # Prediciting and measuring the performace\n",
    "                                y_pred = classifier.predict(x_valid)\n",
    "                                score = accuracy_score(y_valid, y_pred)\n",
    "                                params = {\n",
    "                                    'use_algorithm': use_algorithm,\n",
    "                                    'use_vectorizer': use_vectorizer,\n",
    "                                    'use_stop_words': use_stop_words,\n",
    "                                    'use_alpha': use_alpha,\n",
    "                                    'use_min_df': use_min_df,\n",
    "                                    'use_max_df': use_max_df,\n",
    "                                    'use_smooth_idf': use_smooth_idf,\n",
    "                                    'use_sublinear_tf': use_sublinear_tf\n",
    "                                }\n",
    "\n",
    "                                # Saving the parameters\n",
    "                                model_params.append(params)\n",
    "                                model_score.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Mejor modelo y entrenamiento completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the hiper parameters of the best scored model\n",
    "selected_model_index = np.argmax(model_score)\n",
    "selected_model_score = model_score[selected_model_index]\n",
    "selected_model_params = model_params[selected_model_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_algorithm': 'normal',\n",
      " 'use_alpha': 0.0075,\n",
      " 'use_max_df': 0.15,\n",
      " 'use_min_df': 1,\n",
      " 'use_smooth_idf': False,\n",
      " 'use_stop_words': None,\n",
      " 'use_sublinear_tf': False,\n",
      " 'use_vectorizer': 'TfidfVectorizer'}\n",
      "0.8855501546619532\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(selected_model_params)\n",
    "pprint.pprint(selected_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "classifier = MultinomialNaiveBayes(alpha=selected_model_params['use_alpha'])\n",
    "\n",
    "# Creating the vectorizer\n",
    "if selected_model_params['use_vectorizer'] == 'CountVectorizer':\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=selected_model_params['use_stop_words'], \n",
    "        min_df=selected_model_params['use_min_df'], \n",
    "        max_df=selected_model_params['use_max_df']\n",
    "    )\n",
    "elif selected_model_params['use_vectorizer'] == 'TfidfVectorizer':\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=selected_model_params['use_stop_words'], \n",
    "        min_df=selected_model_params['use_min_df'], \n",
    "        max_df=selected_model_params['use_max_df']\n",
    "    )\n",
    "\n",
    "# Processing the training dataset\n",
    "x_train = vectorizer.fit_transform(_train[selected_model_params['use_algorithm']]['input'])\n",
    "y_train = _train[selected_model_params['use_algorithm']]['output']\n",
    "\n",
    "# Training the model\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Validación y performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8016463090812533\n"
     ]
    }
   ],
   "source": [
    "# Processing the training dataset\n",
    "x_test = vectorizer.transform(test[selected_model_params['use_algorithm']]['input'])\n",
    "y_test = test[selected_model_params['use_algorithm']]['output']\n",
    "\n",
    "# Prediciting with the model\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# Measuring the score\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
