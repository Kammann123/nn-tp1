{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N° 1 - Notebook #2\n",
    "En esta segunda notebook, se busca definir cuál métrica es más apropiada para analizar la performance del modelo y qué hiper parámetros se van a utilizar para el ajuste del modelo acorde a la validación. Finalmente, estas decisiones se vuelcan en la selección del mejor modelo para el problema de la clasificación de correos electrónicos asociados grupos de noticias.\n",
    "\n",
    "### Integrantes del grupo\n",
    "* Kammann, Lucas Agustín\n",
    "* Gaytan, Joaquín Oscar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Métrica\n",
    "La métrica a utilizar para cuantificar la performance de los modelos, seleccionar los hiperparámetros y validarlos será la **exactitud** o **accuracy**.\n",
    "\n",
    "## 1.1 Justificación\n",
    "La problemática a resolver tiene por objetivo asegurar clasificar entre múltiples clases, por ende, el objetivo es acertar la mayor cantidad de predicciones posibles. Para este tipo de problemas, conviene usar la exactitud, pero antes es necesario comprobar que la distribución de clases está balanceada o es uniforme, dado que si no fuera así (o aproximadamente así) entonces habría un sesgo en la estimación. Esto último se debe a que no seríamos capaces de cuantificar realmente lo malo que el modelo es prediciendo aquellas clases minoritarias.\n",
    "\n",
    "Es decir, si bien es una métrica acorde al problema, cuando las clases no están balanceadas su interpretación numérica no es realista. En esos casos, se puede utilizar el promedio de la sensibilidad de cada clase, porque dicha sensibilidad representa la probabilidad de acertar en la predicción dada cada clase y si luego las promediamos estamos ponderando de igual forma cada clase.\n",
    "\n",
    "En conclusión, dado lo que se observó en previos análisis **(ver Notebook #1)**, se puede asumir que la distribución de clases es aproximadamente uniforme por lo cual la exactitud es una métrica aceptable. Si se quisiera obtener una cantidad más realista, con el promedio de sensibilidad se cumpliría tal objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hiper parámetros\n",
    "Se consideran hiper parámetros aquellos que se determinan de manera óptima eligiendo aquel que da mejor resultado en el conjunto de datos de validación, donde muchos modelos con diversos tipos y valores de hiper parámetros compiten por ver cuál obtuvo la mejor medida de performance, es decir, de la métrica. Para ello, consideraremos como hiper parámetros los siguientes aspectos,\n",
    "\n",
    "* Algoritmo de preprocesamiento empleado: Stemming, Lemmatization, Ninguno\n",
    "* Filtrado por stop words\n",
    "* Vectorizer empleado: CountVectorizer, TfIdfVectorizer\n",
    "* Modelo empleado: MultinomialNaiveBayes, OneVsRestClassifier\n",
    "* Coeficiente de Laplacian Smoothing\n",
    "* Coeficiente de mínima frecuencia por documentos\n",
    "* Coeficiente de máxima frecuencia por documentos\n",
    "\n",
    "*NOTA*, sólo correr las siguientes celdas de preprocesamiento, si no se poseen los '*.txt' preprocesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Algoritmos de preprocesamiento\n",
    "A continuación, se corren los algoritmos de procesamiento directamente sobre el conjunto de entreamiento para evitar tener que realizar el preprocesamiento cada vez teniendo en cuenta que no existen más opciones a analizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Descargando el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Train: 11314 elements\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "# Loading the train dataset\n",
    "train = fetch_20newsgroups(\n",
    "    subset='train', \n",
    "    shuffle=True, \n",
    "    remove=('headers', 'footers')\n",
    ")\n",
    "\n",
    "# Train dataset, casting to numpy array\n",
    "train_raw_input = np.array(train.data)\n",
    "train_output = np.array(train.target)\n",
    "train_size = len(train_raw_input)\n",
    "\n",
    "# Logging useful information\n",
    "print(f'Dataset Train: {train_size} elements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Dataset original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal pre processing algorithm finished.\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Process and save the stemmed trainning data\n",
    "normal_train_raw_input = []\n",
    "for document in train_raw_input:\n",
    "    tokens = word_tokenize(document)\n",
    "    new_document = \" \".join([token.lower() for token in tokens if token.isalpha()])\n",
    "    normal_train_raw_input.append(new_document)\n",
    "\n",
    "# Logging\n",
    "print('Normal pre processing algorithm finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normal trainning dataset has been saved in the local storage system.\n",
      "Wall time: 48.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the structure of the trainning data set for the non-processed case\n",
    "normal_train = {\n",
    "    'input': train_raw_input,\n",
    "    'output': train_output\n",
    "}\n",
    "\n",
    "# Save with pickle\n",
    "with open('tp1_ej1_train_normal.txt', 'wb') as file:\n",
    "    pickle.dump(normal_train, file)\n",
    "\n",
    "# Logging\n",
    "print('The normal trainning dataset has been saved in the local storage system.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Dataset con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Lucas A.\n",
      "[nltk_data]     Kammann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lucas A.\n",
      "[nltk_data]     Kammann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming pre processing algorithm finished.\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Instantiate the stemmer instance\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Process and save the stemmed trainning data\n",
    "stemmed_train_raw_input = []\n",
    "for document in train_raw_input:\n",
    "    tokens = word_tokenize(document)\n",
    "    new_document = \" \".join([stemmer.stem(token.lower()) for token in tokens if token.isalpha()])\n",
    "    stemmed_train_raw_input.append(new_document)\n",
    "\n",
    "# Logging\n",
    "print('Stemming pre processing algorithm finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemming trainning dataset has been saved in the local storage system.\n",
      "Wall time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the structure of the trainning data set for the stemming case\n",
    "stemmed_train = {\n",
    "    'input': stemmed_train_raw_input,\n",
    "    'output': train_output\n",
    "}\n",
    "\n",
    "# Save with pickle\n",
    "with open('tp1_ej1_train_stemmed.txt', 'wb') as file:\n",
    "    pickle.dump(stemmed_train, file)\n",
    "\n",
    "# Logging\n",
    "print('The stemming trainning dataset has been saved in the local storage system.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Dataset con lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization pre processing algorithm finished.\n",
      "Wall time: 42.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Instantiate the lemmatizer instance\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Process and save the lemmatized trainning data\n",
    "lemmatized_train_raw_input = []\n",
    "for document in train_raw_input:\n",
    "    tokens = word_tokenize(document)\n",
    "    new_document = \" \".join([lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha()])\n",
    "    lemmatized_train_raw_input.append(new_document)\n",
    "\n",
    "# Logging\n",
    "print('Lemmatization pre processing algorithm finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatization trainning dataset has been saved in the local storage system.\n",
      "Wall time: 72.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the structure of the trainning data set for the lemmatization case\n",
    "lemmatization_train = {\n",
    "    'input': lemmatized_train_raw_input,\n",
    "    'output': train_output\n",
    "}\n",
    "\n",
    "# Save with pickle\n",
    "with open('tp1_ej1_train_lemmatization.txt', 'wb') as file:\n",
    "    pickle.dump(lemmatization_train, file)\n",
    "\n",
    "# Logging\n",
    "print('The lemmatization trainning dataset has been saved in the local storage system.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Train: 11314 elements\n",
      "Dataset Test: 7532 elements\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Loading the datasets\n",
    "train = fetch_20newsgroups(\n",
    "    subset='train', \n",
    "    shuffle=True, \n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "\n",
    "test = fetch_20newsgroups(\n",
    "    subset='test', \n",
    "    shuffle=True, \n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "\n",
    "# Train dataset, casting to numpy array\n",
    "train_raw_input = np.array(train.data)\n",
    "train_output = np.array(train.target)\n",
    "train_size = len(train_raw_input)\n",
    "\n",
    "# Test dataset, casting to numpy array\n",
    "test_raw_input = np.array(test.data)\n",
    "test_output = np.array(test.target)\n",
    "test_size = len(test_raw_input)\n",
    "\n",
    "# Logging useful information\n",
    "print(f'Dataset Train: {train_size} elements')\n",
    "print(f'Dataset Test: {test_size} elements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Preprocessing\n",
    "vectorizer = CountVectorizer(encoding=\"latin1\")\n",
    "train_input = vectorizer.fit_transform(train_raw_input)\n",
    "test_input = vectorizer.transform(test_raw_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.multinomial_naive_bayes import MultinomialNaiveBayes\n",
    "\n",
    "# Training the multinomial naive bayes model\n",
    "classifier = MultinomialNaiveBayes(alpha=0.01)\n",
    "classifier.fit(train_input, train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_input.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(test_output, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = metrics.recall_score(test_output, predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6460435475305364\n",
      "Balanced accuracy: 0.6352908156554821\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Balanced accuracy: {recall}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
